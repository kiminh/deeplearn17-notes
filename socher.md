* 5 Obstacles + tackling
  1.  NLP no single model architecture with state of the art results across many tasks p5
    * the systems in the table may be SOTA _DL_ models, not overall (I'm not
      sure)
  2.  Questions have input independent representations p16
    * dynamic memory model
      Dynamic Coattention Networks For Question Answering, 
      Caiming Xiong, Victor Zhong, Richard Socher
      ICLR 2017 
    * [blog](http://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering)
  3.  RNNs are Slow
    * Quasi-Recurrent Neural Networks, 
    James Bradbury, Stephen Merity, Caiming Xiong, Richard Socher
    ICLR 2017
    * az RNN és a CNN bizonyos tulajdonságait egyesítő modell szövegre
    * [blog](https://einstein.ai/research/new-neural-network-building-block-allows-faster-and-more-accurate-text-understanding) 
  4.  Hard to generate long language sequences that make sense
    * A Deep Reinforced Model for Abstractive Summarization, 
    Romain Paulus, Caiming Xiong, Richard Socher
    * [blog post](https://einstein.ai/research/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization), és 
  5.  Joint Many-task Learning p32
    * A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,
    Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher
    EMNLP 2017. Also appeared in a NIPS 2016 Workshop
    * [blog post](https://einstein.ai/research/multiple-different-natural-language-processing-tasks-in-a-single-deep-model)
    * Chunking training, Entailment  training p35
    * POS tagging, Chunking, Semantic relatedness, Dependency, Textual entailment,
