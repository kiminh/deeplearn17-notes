#I: Basics of Machine Learning and Applications --- deep and shallow

* ML founding principles
* ML and deep learning
* shallow ML vs. deep ML
* taxonomy of ML: a learning-paradigm perspective
* taxonomy of speech, image, text, and multi-modal applications: a
  signal-processing perspective

#II: Deep Neural Networks (DNN): Why gradient vanishes & how to rescue it

* history of neural nets for speech recognition: why they failed
* one equation for backprop update --- why gradients may easily vanish for DNN
  learning
* five ways of rescuing gradient vanishing
* an alternative way of training DNN (deep stacking net)
* recurrent nets: my experiments in 90s (for speech) and current perspectives

#III: How Deep Learning Disrupted Speech (and Image) Recognition

* shallow models dominating speech: 30+ years from 80s
* deep generative models for speech: 10 years of research before DNN disruption
* pros and cons of generative vs discriminative models
* how speech is produced and perceived by human: a comprehensive computational
  model
* several theories of human perception
* variational inference/learning for deep generative speech model (experiments
  late 90's to mid 2000)
* a very different kind of deep generative model: deep belief nets (2006)
* the arrival of DNN for speech and its early successes: a historical
  perspective (2009-2011)
* more recent development of deep learning for speech
* a perspective on recent innovations in speech recognition
* how to do truly unsupervised learning for future speech recognition (and
  other AI tasks)

#IV: Deep Learning for Text and Multi-Modal Processing

* AI to move from perception to cognition: key roles of language/text
* concept of symbolic/semantic embedding
* word and text embedding
* build text embedding on top of sub-word units: practical necessity for many
  applications
* distant supervised embedding
* deep structured semantic modeling (DSSM)
* use of DSSM for multi-modal deep learning: Microsoft's first generation image
  captioning system
* DSSM for contextual search in Microsoft Office/Word

#V: Limitations of Current Deep Learning and How to Overcome Them

* Interpretability problem
* Symbolic-neural integration for reasoning: tensor-product representations
* How do labels come from: the need for 
  * unsupervised learning via rich priors and self learning via interactions
* Vertical applications 
